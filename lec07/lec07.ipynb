{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42278198",
   "metadata": {},
   "source": [
    "## Подбор параметров модели методом поиска на сетке - GridSearch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40abd4",
   "metadata": {},
   "source": [
    "Поиск оптимальных значений ключевых параметров модели (то есть значений, которые дают наилучшую обобщающую\n",
    "способность) является сложной задачей, но она обязательна почти для всех моделей и наборов данных. Поскольку поиск оптимальных значений параметров является общераспространенной задачей, библиотека **scikit-learn** предлагает стандартные методы, позволяющие решить ее.\n",
    "\n",
    "Наиболее часто используемый метод – это решетчатый поиск (**grid search**), который по сути является перебором всех возможных комбинаций интересующих параметров.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f0349",
   "metadata": {},
   "source": [
    "Рассмотрим модель дерева решений DecisionTreeClassifier на известном наборе данных и оценим качество классификации при различных значениях параметров модели:\n",
    "\n",
    "* min_samples_leaf - минимальное кол-во примеров в листе (по умолчанию 1)\n",
    "* min_samples_split - минимальное кол-во примеров для ветвления (по умолчанию 2)\n",
    "* max_depth - максимальная глубина дерева (по умоланию не ограничено)\n",
    "\n",
    "Значения по умолчанию часто приводят к заметному перебочунию дерева решений (на обучающей выборке 1.0, а на тестовой значительно ниже)\n",
    "\n",
    "Покрутим эти параметры по сетке из следующих значений:\n",
    "\n",
    "* min_samples_leaf = {1, 2, ..10}\n",
    "* min_samples_split = {2,4, ... 10}\n",
    "* max_depth = {2,3,...20}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c624",
   "metadata": {},
   "source": [
    "Класс **GridSearchCV** выполняет подбор параметров указанной модели на заданной сетке значений с кросс-валидацией модели, поскольку хорошие результаты для набора параметров могли быть получены на случайно удачном наборе данных, потому для проверки помимо разбиения на train/test для каждой модели необходимо также выделить валидационную часть общего набора данных, либо выполнять кросс-валидацию, которую **GridSearchCV** и выполняет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5a2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa1205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(2, 20),\n",
       "                         'min_samples_leaf': range(1, 10),\n",
       "                         'min_samples_split': range(2, 10, 2)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "params = { 'max_depth': range (2, 20, 1),\n",
    "              'min_samples_leaf': range (1, 10),\n",
    "              'min_samples_split': range (2,10,2) }\n",
    "\n",
    "grid = GridSearchCV(dtc, params, cv = 5, n_jobs = -1)\n",
    "grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf6ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения параметров: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "Наилучшее значение метрики: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучшие значения параметров: {grid.best_params_}\")\n",
    "print(f\"Наилучшее значение метрики: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b53cd",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9251d8f",
   "metadata": {},
   "source": [
    "## Ансамбли моделей. Случайный лес (RandomForestClassifier)\n",
    "\n",
    "В машинном обучении под ансамблем моделей понимают комбинацию нескольких алгоритмов обучения, которые, работая вместе, позволяют построить модель более эффективную и точную, чем любая из моделей, построенная с помощью отдельного алгоритма.\n",
    "\n",
    "случайный лес – это набор деревьев решений, где каждое дерево немного отличается от остальных. Идея случайного леса заключается в том, что каждое дерево может довольно хорошо прогнозировать, но скорее всего переобучается на части данных. Если мы построим много деревьев, которые хорошо работают и переобучаются с разной степенью, мы можем уменьшить переобучение путем усреднения их результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e080df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1304e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 5)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f850ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на обучающем наборе: 0.991\n",
      "Точность на тестовом наборе: 0.974\n"
     ]
    }
   ],
   "source": [
    "print(\"Точность на обучающем наборе: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Точность на тестовом наборе: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735c1063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(n_estimators=5), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(2, 20),\n",
       "                         'min_samples_leaf': range(1, 10),\n",
       "                         'min_samples_split': range(2, 10, 2),\n",
       "                         'n_estimators': range(3, 20)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {  \"n_estimators\": range(3,20),\n",
    "            'max_depth': range (2, 20, 1),\n",
    "            'min_samples_leaf': range (1, 10),\n",
    "            'min_samples_split': range (2,10,2) }\n",
    "\n",
    "gridRf = GridSearchCV(forest, params, cv = 5, n_jobs = -1)\n",
    "gridRf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201e1087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения параметров: {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 3}\n",
      "Наилучшее значение метрики: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучшие значения параметров: {gridRf.best_params_}\")\n",
    "print(f\"Наилучшее значение метрики: {gridRf.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30cb08",
   "metadata": {},
   "source": [
    "## Ансамбли моделей. Бэггинг (Bagging) и Бустинг (Boosting)\n",
    "\n",
    "\n",
    "Помимо случайного леса существует несколько типов ансамблей моделей, но два наиболее распространенных - это бэггинг и бустинг.\n",
    "\n",
    "Бэггинг (Bagging) - это метод, который основан на создании нескольких независимых моделей на основе разных подмножеств обучающих данных. Каждая модель обучается на своем подмножестве данных и в результате выдает свой прогноз. Затем, прогнозы всех моделей усредняются, чтобы получить окончательный результат. Использование бэггинга позволяет уменьшить влияние шумовых данных и увеличить стабильность и точность предсказаний.\n",
    "\n",
    "Бустинг (Boosting) - это метод, который похож на бэггинг, но с одним основным отличием: модели создаются последовательно, и каждая следующая модель использует информацию об ошибках предыдущей модели для улучшения своих прогнозов. Это позволяет уменьшить ошибки, которые делает каждая модель, и увеличить точность предсказаний.\n",
    "\n",
    "Кроме того, существуют и другие типы ансамблей моделей, такие как случайный лес (Random Forest), который является вариантом бэггинга, но использует решающие деревья в качестве базовых моделей, и градиентный бустинг (Gradient Boosting), который является одной из разновидностей бустинга, но использует градиентный спуск для минимизации ошибок.\n",
    "\n",
    "В целом, использование ансамблей моделей может значительно улучшить качество предсказаний в машинном обучении, особенно если выбранные модели являются достаточно разнообразными и хорошо настроены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea8093",
   "metadata": {},
   "source": [
    "### BaggingClassifier\n",
    "\n",
    "**BaggingClassifier** - это классификатор на основе бэггинга, доступный в библиотеке **scikit-learn**. Он позволяет создавать ансамбли моделей на основе разных алгоритмов машинного обучения с использованием бэггинга.\n",
    "\n",
    "\n",
    "Основные параметры: \n",
    "* base_estimator - это базовый алгоритм, на основе которого создается ансамбль (в данном случае -  К ближайших соседей), \n",
    "* n_estimators - количество моделей в ансамбле, \n",
    "* random_state - начальное значение для генератора случайных чисел, чтобы обеспечить воспроизводимость результатов.\n",
    "\n",
    "**BaggingClassifier** используется с различными алгоритмами машинного обучения в качестве базовых моделей. Например, вместо решающего дерева можно использовать логистическую регрессию или метод ближайших соседей. \n",
    "\n",
    "Важно помнить, что выбор базового алгоритма должен основываться на особенностях конкретной задачи и на том, какой тип модели лучше всего подходит для решения этой задачи.\n",
    "\n",
    "Рассмотрим пример использования классификатора на основе алгоритма К ближайших соседей:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873eea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# создаем синтетические данные для примера\n",
    "X, y = make_classification( n_samples = 1000, n_features = 10 )\n",
    "\n",
    "# разделяем данные на обучающие и тестовые наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2 )\n",
    "\n",
    "# создаем базовый алгоритм KNeighborsClassifier\n",
    "base_model = KNeighborsClassifier()\n",
    "\n",
    "# создаем ансамбль на основе базового алгоритма с использованием бэггинга\n",
    "bagging_model = BaggingClassifier( base_estimator = base_model, n_estimators = 10, random_state = 42 )\n",
    "\n",
    "# обучаем модель на обучающих данных\n",
    "bagging_model.fit( X_train, y_train )\n",
    "\n",
    "# делаем предсказания на тестовых данных\n",
    "y_pred = bagging_model.predict( X_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d225a1f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Градиентный бустинг. BoostingClassifier\n",
    "\n",
    "**BoostingClassifier** - это алгоритм ансамбля машинного обучения, который комбинирует несколько слабых моделей, чтобы получить более сильную модель. Он работает путем последовательного обучения моделей на разных подмножествах данных, с тем чтобы каждая последующая модель сконцентрировалась на тех объектах, на которых предыдущие модели ошибались. В итоге получается ансамбль, который показывает более высокую точность предсказаний, чем отдельные модели.\n",
    "\n",
    "Рассмотрим пример, аналогичный Бэггингу с испольщзованием алгоритма AdaBoosting для дерева решений:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbda3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# создаем синтетические данные для примера\n",
    "X, y = make_classification( n_samples = 1000, n_features = 10 )\n",
    "\n",
    "# разделяем данные на обучающие и тестовые наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2 )\n",
    "\n",
    "# создаем базовый алгоритм DecisionTreeClassifier\n",
    "base_model = DecisionTreeClassifier()\n",
    "\n",
    "# создаем ансамбль на основе базового алгоритма с использованием boosting\n",
    "boosting_model = AdaBoostClassifier( base_estimator = base_model, n_estimators = 10, random_state = 42 )\n",
    "\n",
    "# обучаем модель на обучающих данных\n",
    "boosting_model.fit( X_train, y_train )\n",
    "\n",
    "# делаем предсказания на тестовых данных\n",
    "y_pred = boosting_model.predict( X_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04f9db",
   "metadata": {},
   "source": [
    "помимо **AdaBoostClassifier** можно воспользоваться **GradientBoostingClassifier**. Это два разных алгоритма градиентного бустинга, которые используются для построения ансамблей моделей машинного обучения.\n",
    "\n",
    "Основное отличие между ними заключается в том, как они обновляют веса объектов в процессе построения ансамбля.\n",
    "\n",
    "**AdaBoostClassifier** обновляет веса объектов на каждой итерации, увеличивая веса объектов, на которых были сделаны ошибки, и уменьшая веса объектов, на которых были сделаны правильные предсказания. Новый базовый алгоритм строится на основе этих обновленных весов. В итоге, объекты, на которых было сделано больше ошибок, получают более высокий вес, и следующая модель в ансамбле более фокусируется на этих объектах.\n",
    "\n",
    "**GradientBoostingClassifier** также обновляет веса объектов, но делает это путем минимизации функции потерь, используя градиентный спуск. В отличие от ***AdaBoostClassifier**, **GradientBoostingClassifier** не просто корректирует веса объектов, но также учитывает градиенты функции потерь, чтобы найти наилучшее направление обновления весов. Базовые алгоритмы в **GradientBoostingClassifier** строятся на основе этих новых весов, и ансамбль строится путем последовательного добавления базовых алгоритмов с учетом их вклада в функцию потерь.\n",
    "\n",
    "Таким образом, **AdaBoostClassifier** и **GradientBoostingClassifier** оба используют градиентный спуск, чтобы строить ансамбли моделей, но делают это с разной целью и способом обновления весов объектов. \n",
    "* **AdaBoostClassifier** сосредотачивается на тех объектах, на которых было сделано больше ошибок, в то время как \n",
    "* **GradientBoostingClassifier** учитывает градиенты функции потерь, чтобы наилучшим образом обновить веса объектов и улучшить общее качество модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
